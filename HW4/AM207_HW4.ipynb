{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework #4\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Thursday, Febrary 23rd, 2017 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Optimization (Continued)\n",
    "\n",
    "Suppose you are building a pricing model for laying down telecom cables over a geographical region. Your model takes as input a pair of  coordinates, $(x, y)$, and contains two parameters, $\\lambda_1, \\lambda_2$. Given a coordinate, $(x, y)$, and model parameters, the loss in revenue corresponding to the price model at location $(x, y)$ is described by\n",
    "$$\n",
    "L(x, y, \\lambda_1, \\lambda_2) = 0.000045\\lambda_2^2 y - 0.000098\\lambda_1^2 x - 0.0005\\lambda_1 x\\exp\\left\\{\\left(y^2 - x^2\\right)\\left(\\lambda_1^2 + \\lambda_2^2\\right)\\right\\}\n",
    "$$\n",
    "Read the data contained in `HW3_data.csv`. This is a set of coordinates configured on the curve $y^2 - x^2 = -0.1$. Given the data, find parameters $\\lambda_1, \\lambda_2$ that minimize the net loss over the entire dataset.\n",
    "\n",
    "### Part A: Further problems with descent algorithms\n",
    "Using your implementation of gradient descent and stochastic gradient descent, document the behaviour of your two algorithms for the following starting points, and for a number of stepsizes of your choice:\n",
    "- $(\\lambda_1, \\lambda_2) = (-2.47865, 0)$\n",
    "- $(\\lambda_1, \\lambda_2) = (-3, 0)$\n",
    "- $(\\lambda_1, \\lambda_2) = (-5, 0)$\n",
    "- $(\\lambda_1, \\lambda_2) = (-10, 0)$\n",
    "Based on your analysis of the lost function $L$, explain what is happening to your descent algorithms.\n",
    "\n",
    "### Part B: Simulated Annealing\n",
    "Implement Simulated Annealing initalized at $(\\lambda_1, \\lambda_2) = (-5, 0)$ to minimize our lost function $L$. Compare your results to what you obtained for gradient descent and stochastic gradient descent initialized at $(\\lambda_1, \\lambda_2) = (-5, 0)$.\n",
    "\n",
    "For your Simulated Annealing implementation, we suggest *starting* with following settings for parameters (you should further experiment with and tweak these or feel free to set your own):\n",
    "\n",
    "- Proposal distribution: bivariate normal with covariance $[[1, 0], [0, 1]]$\n",
    "- Min Length: 500\n",
    "- Max Temperature: 10\n",
    "\n",
    "You should also set your own cooling schedule.\n",
    "\n",
    "**Extra Credit**\n",
    "For each temperature, plot the parameters accepted or the cost function with respect to the iteration number. What is happening to the these parameters or costs over iterations? Connect the trends you observe in the visualization to the lecture on Markov Chains."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
